{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjamMI6R4J94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f927aee9-9e50-436e-85d9-a5d5a2eb1abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The more efficient and cost-effective clinical approach to BP lowering is to base treatment decisions on an estimate of an individual’s short-term absolute CVD risk rather than with BP based strategy.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# 读取CSV文件\n",
        "df = pd.read_csv('/content/abstract_clean.csv')\n",
        "\n",
        "# 选择某一列\n",
        "selected_column = df['Abstract']  # 将'column_name'替换为你想要选择的列的名称\n",
        "\n",
        "# 打印或进一步处理选定的列\n",
        "#print(selected_column)\n",
        "\n",
        "# 将内容按句子拆分\n",
        "sentences = []\n",
        "for text in selected_column:\n",
        "    # 使用正则表达式拆分句子\n",
        "    sentences.extend(re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', str(text)))\n",
        "\n",
        "# 将拆分后的内容存储为txt文件\n",
        "with open('output.txt', 'w', encoding='utf-8') as file:\n",
        "    for sentence in sentences:\n",
        "        file.write(sentence + '\\n')\n",
        "\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将句子按单词拆分，并以BIO格式存储\n",
        "with open('output_bio.txt', 'w', encoding='utf-8') as file:\n",
        "    for sentence in sentences:\n",
        "        # 拆分单词\n",
        "        words = sentence.split()\n",
        "\n",
        "        # 写入BIO格式\n",
        "        for i, word in enumerate(words):\n",
        "            if i == 0:\n",
        "                file.write(word + \" O\\n\")  # 第一个单词为Begin\n",
        "            else:\n",
        "                file.write(word + \" O\\n\")  # 后续单词为Inside\n",
        "        file.write(\"\\n\")  # 空行表示句子结束"
      ],
      "metadata": {
        "id": "83Y222WIsCk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labels_from_bio(file_path):\n",
        "    labels = set()\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                _, label = line.split(' ')\n",
        "                labels.add(label)\n",
        "\n",
        "    return labels\n",
        "\n",
        "# 替换为你的BIO格式文件路径\n",
        "bio_file_path = '/content/train.txt'\n",
        "\n",
        "# 提取所有存在的标签\n",
        "all_labels = extract_labels_from_bio(bio_file_path)\n",
        "\n",
        "# 打印所有存在的标签\n",
        "print(\"所有存在的标签:\")\n",
        "for label in all_labels:\n",
        "    print(label)\n"
      ],
      "metadata": {
        "id": "1I8RzZJBs54U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/paragraph.zip"
      ],
      "metadata": {
        "id": "1ANjA-6_BjKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 指定BIO格式文档所在的文件夹路径\n",
        "folder_path = '/content/paragraph'\n",
        "\n",
        "# 列出文件夹中的所有文档\n",
        "documents = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "# 定义标签列表\n",
        "labels = [\"I-Position\", \"I-SBP\", \"I-MBP\", \"I-Age\", \"I-SBP_SLASH_DBP\", \"I-DBP\", \"I-Comorbidity\",\n",
        "          \"I-Height\", \"I-Location\", \"I-mmHg\", \"I-Race\", \"I-Sex\", \"I-Method\"]\n",
        "\n",
        "# 创建一个DataFrame用于保存结果\n",
        "columns = ['Document']\n",
        "columns.extend(labels)\n",
        "result_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# 遍历每个文档\n",
        "for document in documents:\n",
        "    # 读取BIO格式的文档内容\n",
        "    with open(os.path.join(folder_path, document), 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # 初始化行数据\n",
        "    row_data = {'Document': document}\n",
        "    for label in labels:\n",
        "        row_data[label] = []\n",
        "\n",
        "    # 提取每个标签对应的单词或词组\n",
        "    current_label = ''\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            print(line)\n",
        "            word, label0, label = line.split(' ')\n",
        "            if label in labels:\n",
        "                current_label = label\n",
        "                row_data[current_label].append(word)\n",
        "        else:\n",
        "            # 处理空行，表示句子结束，重置当前标签\n",
        "            current_label = ''\n",
        "\n",
        "    # 将行数据添加到DataFrame\n",
        "    result_df = result_df.append(row_data, ignore_index=True)\n",
        "\n",
        "# 将结果保存为CSV文件\n",
        "result_df.to_csv('output1.csv', index=False)\n"
      ],
      "metadata": {
        "id": "2cCYmOWUBTiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 指定BIO格式文档所在的文件夹路径\n",
        "folder_path = '/content/paragraph'\n",
        "\n",
        "# 列出文件夹中的所有文档，并按照数字进行排序\n",
        "documents = sorted([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))],\n",
        "                   key=lambda x: int(x.split('.')[0]))\n",
        "\n",
        "# 定义标签列表\n",
        "labels = [\"I-Position\", \"I-SBP\", \"I-MBP\", \"I-Age\", \"I-SBP_SLASH_DBP\", \"I-DBP\", \"I-Comorbidity\",\n",
        "          \"I-Height\", \"I-Location\", \"I-mmHg\", \"I-Race\", \"I-Sex\", \"I-Method\"]\n",
        "\n",
        "# 创建一个DataFrame用于保存结果\n",
        "columns = ['Document']\n",
        "columns.extend(labels)\n",
        "result_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# 遍历每个文档\n",
        "for document in documents:\n",
        "    # 读取BIO格式的文档内容\n",
        "    with open(os.path.join(folder_path, document), 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # 初始化行数据\n",
        "    row_data = {'Document': document}\n",
        "    for label in labels:\n",
        "        row_data[label] = []\n",
        "\n",
        "    # 提取每个标签对应的单词或词组\n",
        "    current_label = ''\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            word, label1, label = line.split(' ')\n",
        "            if label in labels:\n",
        "                current_label = label\n",
        "                row_data[current_label].append(word)\n",
        "        else:\n",
        "            # 处理空行，表示句子结束，重置当前标签\n",
        "            current_label = ''\n",
        "\n",
        "    # 将行数据添加到DataFrame\n",
        "    result_df = result_df.append(row_data, ignore_index=True)\n",
        "\n",
        "# 将结果保存为CSV文件\n",
        "result_df.to_csv('output.csv', index=False)\n"
      ],
      "metadata": {
        "id": "GzEcbEdSlsHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 指定两个文本文件的路径\n",
        "file1_path = '/content/test.txt'\n",
        "file2_path = '/content/predictions-EntLM-round4.txt'\n",
        "output_file_path = 'output.txt'\n",
        "\n",
        "# 读取第一个文本文件的第一列\n",
        "with open(file1_path, 'r') as file1:\n",
        "    column1_file1 = [line.split()[0] for line in file1]\n",
        "\n",
        "# 读取第二个文本文件的第三列\n",
        "with open(file2_path, 'r') as file2:\n",
        "    column3_file2 = [line.split()[2] for line in file2]\n",
        "\n",
        "# 合并两列数据\n",
        "combined_data = [f\"{col1} {col3}\\n\" for col1, col3 in zip(column1_file1, column3_file2)]\n",
        "\n",
        "# 将合并后的数据写入新的文本文件\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    output_file.writelines(combined_data)\n"
      ],
      "metadata": {
        "id": "cc4JwNvnVFuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 指定两个文本文件的路径\n",
        "file1_path = '/content/test.txt'\n",
        "file2_path = '/content/predictions-EntLM-round4.txt'\n",
        "output_file_path = 'output.txt'\n",
        "\n",
        "# 读取两个文本文件的内容\n",
        "with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
        "    lines_file1 = file1.readlines()\n",
        "    lines_file2 = file2.readlines()\n",
        "\n",
        "# 合并两个文档的指定列并写入新的文本文件\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for line1, line2 in zip(lines_file1, lines_file2):\n",
        "        line1 = line1.strip()\n",
        "        line2 = line2.strip()\n",
        "\n",
        "        if line1 and line2:  # 如果两行都不是空行\n",
        "            col1 = line1.split()[0]\n",
        "            col3 = line2.split()[2]\n",
        "            output_file.write(f\"{col1} {col3}\\n\")\n",
        "        elif line1 or line2:  # 如果其中一行不是空行\n",
        "            output_file.write('\\n')  # 保留空行\n"
      ],
      "metadata": {
        "id": "E4P7gIzpVnuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 指定两个文本文件的路径\n",
        "file1_path = '/content/test.txt'\n",
        "file2_path = '/content/predictions-EntLM-round4.txt'\n",
        "output_file_path = 'output.txt'\n",
        "\n",
        "# 读取两个文本文件的内容\n",
        "with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
        "    lines_file1 = file1.readlines()\n",
        "    lines_file2 = file2.readlines()\n",
        "\n",
        "# 合并两个文档的指定列并写入新的文本文件\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for line1, line2 in zip(lines_file1, lines_file2):\n",
        "        line1 = line1.strip()\n",
        "        line2 = line2.strip()\n",
        "\n",
        "        if line1 and line2:  # 如果两行都不是空行\n",
        "            col1 = line1.split()[0]\n",
        "            col3 = line2.split()[2]\n",
        "            output_file.write(f\"{col1} {col3}\\n\")\n",
        "        elif line1 or line2:  # 如果其中一行不是空行\n",
        "            output_file.write('\\n')  # 保留空行\n",
        "        else:  # 如果两行都是空行\n",
        "            output_file.write('\\n')  # 保留空行\n"
      ],
      "metadata": {
        "id": "9J-t-LYzV3iG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}